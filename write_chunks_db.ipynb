{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cthewolf/Installations/anaconda3/envs/clyde/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re, pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils import strip_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords-en.txt', mode='r') as f:\n",
    "    stopwords = set([line.strip('\\n') for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = Counter()\n",
    "for article in tqdm(ds):\n",
    "    total_counts.update(re.split(r'[\\n| ]+', strip_punc(article['text']).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_counts.pkl', mode='wb') as f:\n",
    "    pickle.dump(total_counts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to restart from this point\n",
    "# with open('total_counts.pkl', mode='rb') as f:\n",
    "#     total_counts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total_counts.total()\n",
    "inverse_frequencies = {word: round(total / count) for word, count in total_counts.items()\n",
    "                        if word not in stopwords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inverse_frequencies.pkl', mode='wb') as f:\n",
    "    pickle.dump(inverse_frequencies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to restart from this point\n",
    "with open('inverse_frequencies.pkl', mode='rb') as f:\n",
    "    inverse_frequencies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31142/6407814 [04:31<16:26:23, 107.74it/s]"
     ]
    }
   ],
   "source": [
    "MIN_LENGTH = 20\n",
    "NUM_KEYWORDS = 10\n",
    "def reweight(counts):\n",
    "    return Counter({k: v * inverse_frequencies[k] for k, v in counts.items()\n",
    "                    if k not in stopwords})\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=postgres user=postgres\")\n",
    "cur = conn.cursor()\n",
    "for article in tqdm(ds):\n",
    "    # collect keywords for each chunk in the article\n",
    "    chunks = [para.split(' ') for para in strip_punc(article['text']).lower().split('\\n')]\n",
    "    counters = [reweight(Counter(chunk)) for chunk in chunks]\n",
    "    wikichunks_rows = []\n",
    "    keywords_by_chunk = []\n",
    "    for i, counter in enumerate(counters):\n",
    "        if len(chunks[i]) < MIN_LENGTH:\n",
    "            continue\n",
    "        keywords_by_chunk.append([k for k, _ in counter.most_common(NUM_KEYWORDS)])\n",
    "        wikichunks_rows.append((article['id'], i, article['title']))\n",
    "\n",
    "    # add data to wikichunks table\n",
    "    wikichunks_query = '''\n",
    "        INSERT INTO wikichunks\n",
    "        VALUES %s\n",
    "        RETURNING id;\n",
    "    '''\n",
    "    chunk_ids = execute_values(cur, wikichunks_query, wikichunks_rows, '(DEFAULT, %s, %s, %s)', fetch=True)\n",
    "\n",
    "    # map keywords to chunks\n",
    "    keywords_rows = defaultdict(list)\n",
    "    for (chunk_id,), keywords in zip(chunk_ids, keywords_by_chunk):\n",
    "        for word in keywords:\n",
    "            keywords_rows[word].append(chunk_id)\n",
    "    keywords_query = '''\n",
    "        INSERT INTO keywords\n",
    "        VALUES %s\n",
    "        ON CONFLICT (word_hash)\n",
    "        DO UPDATE SET chunks = keywords.chunks || EXCLUDED.chunks;\n",
    "    '''\n",
    "    execute_values(cur, keywords_query, [(hash(k), k, v) for k, v in keywords_rows.items()])\n",
    "    conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clyde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
